# 2장 간단한 분류 알고리즘 훈련

- 이장에서는 아주 기본적인 머신러닝 알고리즘인 퍼셉트론과 선형 뉴런 두개를 사용하고, 이를 구현하고 붓꽃데이터를 통해 훈련하여 꽃 품종을 분류한다. 

이 장에서는 다음 주제를 다룬다.


1. 머신러닝 알고리즘 직관적으로 이해하기
2. 판다스, 넘파이, 맷플롯립으로 데이터를 읽고 처리하고 시각화하기
3. 파이썬으로 선형 분류 알고리즘 구현하기

## 2.1 인공뉴런: 초기 머신러닝의 간단한 역사

- 초창기에 사람의 뇌 속 신경망 구조를 기반으로 퍼셉트론 학습 개념을 만들었다. 기존의 뇌 속 뉴런은 수상 돌기에 여러 신호가 도착하면 세포체에 합쳐지고, 합쳐진 신호가 임계값을 넘으면 출력신호가 생성되고 축삭돌기를 통해 전달된다.


- 이러한 신경망 구조를 바탕으로 인공 신경망을 만들었다. 입력 데이터가 도착하면 각각 가중치와 곱해진 후 모두 더하고 여기에 편향 값을 더해준다. 이 값에 활성화 함수 취해주고, 값을 출력한다.

![image.png](attachment:image.png)

### 2.1.1 인공 뉴런의 수학적 정의 

- 인공 뉴런 아이디어를 두개의 클래스 레이블로 분류하는 이진 분류작업으로 예를 들자. 두 클래스는 양성클래스(+1)과 음성클래스(-1)로 나타낸다. 그리고 입력값은 x, 가중치 벡터는 w의 선형 조합으로 결정함수$\Phi$(z), 최종 출력은 $z=w_1x_1+...+w_mx_m$으로 정의한다.(z는 점곱으로 표시할 수 있다.)

*x와 v의 곱을 행렬 곱으로 표현하려면 행과 열을 바꿔주는 전치가 필요하다.*

$$w = \begin{bmatrix}
w_1 \\
\vdots \\
w_m
\end{bmatrix} 
, x = \begin{bmatrix}
x_1 \\
\vdots \\
x_m
\end{bmatrix}$$

- 가중치벡터와 입력값의 곱을 모두 더한것이 임계값을 넘으면 양성 클래스 아니면 음성 클래스로 예측한다.

$$\Phi(z)= \begin{cases}
1   z\ge \theta \\
-1    그 외
\end{cases}$$

- 식을 간단하게 만들기 위해서 편향값을 더해 임계값을 0에 위치하게 만들어 준다. $z=w_1x_1+...+w_mx_m + w_0(가중치)$ (이때 편향값은 $-\theta$). 이식에 대해서는 다음과 같이 예측한다.

$$\Phi(z)= \begin{cases}
1   z\ge 0 \\
-1    그 외
\end{cases}$$

![image.png](attachment:image.png)

![image.png](attachment:image.png)

### 2.1.2 퍼셉트론 학습 규칙 

초기 퍼셉트론 학습 규칙은 다음과정과 같다.

1. 가중치를 0 또는 랜덤한 작은 값으로 초기화한다.
2. 각 훈련 샘플 $x^{(i)}$에서 다음 작업을 한다
- 2-1 출력값 $\hat{y}$를 계산한다
- 2-2 가중치를 업데이트한다

가중치 업데이트는 다음과정으로 진행된다.

$$w_j=w_j+\Delta w_j$$

$$\Delta w_j=\eta (y^{(i)}-\hat{y^{(i)}})x_j^{(i)}$$

- 여기서 $\eta$는 학습률이다. 이거는 학습하는 정도를 설정하여 준다. 

- $y^{(i)}$는 진짜 클래스 레이블 쉽게 말해 정답값이다.
- $\hat{y^{(i)}}$는 모델로부터 나온 레이블인 예측 클래스 레이블이다. 




- 가중치를 업데이트할때 자기가 낸 정답과 진짜 정답의 차이를 이용하여 가중치를 업데이트한다.




![image.png](attachment:image.png)

- 가중치를 업데이트는 절대로 멈출일이 없다. 그렇기 때문에 몇 번의 학습을 할 것인지 정해주어야 한다. 이 학습 횟수를 epoch(에포크)라고 한다. 좋은 모델을 구현하려면 학습 횟수는 적은데 정확도가 높은 모델을 생성하는 것이다.

## 2.2 파이썬으로 퍼셉트론 학습 알고리즘 구현

- 이번장에서는 라이브러리를 사용하지 않고 퍼셉트론 학습 알고리즘 구현을 해볼 것이다.

### 2.2.1 객체 지향 퍼셉트론 API

- 라이브러리를 사용할 때 perceptron 객체를 초기화 시키고, fit 메서드로 데이터를 통해 학습하고, 별도의 predict 메서드로 예측을 만든다. 이과정을 손으로 작성해 보자.


```python
import numpy as np #numpy는 수학적인 배열이나 리스틈를 만들어 주는 역할


class Perceptron(object):

    def __init__(self, eta=0.01, n_iter=50, random_state=1): #init 초기값을 설정해주는 함수
        # 학습률, epoch를 설정하고 시드를 설정 
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y): #머신러닝 학습해주는 과정: fit
        # X는 데이터의 특징, y는 클래스(정답)
        rgen = np.random.RandomState(self.random_state)
        # 정규분포에서 랜덤한 숫자를 뽑는 함수
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        # SSE를 계산하기 위해 각각의 에러를 저장할 리스트 초기화
        self.errors_ = []
        
        # 학습하는 과정
        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi)) #정답값에 학습률을 빼준다
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0) # 업데이트가 0이면 에러난게 아님
            self.errors_.append(errors)
        return self

    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.net_input(X) >= 0.0, 1, -1) #np.where을 사용해 양성값인지 음성값인지 구분..?
```


```python
v1 = np.array([1, 2, 3])
v2 = 0.5 * v1
# 역코사인 삼각함수를 나타내고, np.linalg.norm은 벡터길이를 계산하기 위한 함수, .dot는 elemental wise 곱이다.
np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
```




    0.0



#### self 먼지 공부하기

### 2.2.2 붓꽃 데이터셋에서 퍼셉트론 훈련 

- 이전의 세게의 클래스를 가진 붓꽃 데이터셋에서 두개의 클래스만을 이용하고, 여라가지 특성중 두가지 특성만을 사용하여 퍼셉트론 학습에 사용하여 보자

- 먼저 pandas 라이브러리로 주소를 통한 데이터셋을 호출 및 저장 해주자.


```python
import pandas as pd #ㅔ\pandas는 데이터 저장? 그쪽에서 많이 사용
#csv파일을 읽어오는 함수
df = pd.read_csv('https://archive.ics.uci.edu/ml/'
        'machine-learning-databases/iris/iris.data', header=None)
#읽어들인 파일의 제일 마지막거 5개를 불러온다
df.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>145</th>
      <td>6.7</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>146</th>
      <td>6.3</td>
      <td>2.5</td>
      <td>5.0</td>
      <td>1.9</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>147</th>
      <td>6.5</td>
      <td>3.0</td>
      <td>5.2</td>
      <td>2.0</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>148</th>
      <td>6.2</td>
      <td>3.4</td>
      <td>5.4</td>
      <td>2.3</td>
      <td>Iris-virginica</td>
    </tr>
    <tr>
      <th>149</th>
      <td>5.9</td>
      <td>3.0</td>
      <td>5.1</td>
      <td>1.8</td>
      <td>Iris-virginica</td>
    </tr>
  </tbody>
</table>
</div>



- 데이터를 불러온 다음 setosa와 versicolor 클래스에 해당되는 데이터를 추출하고, 이를 정수클래스 1,-1로 바꾼후 y에 저장한다. 특성도 2개를 가져와 값만을 저장한다. 

- 가저온 데이터에 대해서 산점도로 시각화해보면 다음과 같다.


```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# setosa와 versicolor를 선택합니다
y = df.iloc[0:100, 4].values #4번째의 값만 가져다줌
y = np.where(y == 'Iris-setosa', -1, 1)

# 꽃받침 길이와 꽃잎 길이를 추출합니다
X = df.iloc[0:100, [0, 2]].values#0-100 째의 샘플중에서 0번째행과 2번째행의 데이터만 가져와 준다

# 산점도를 그립니다
plt.scatter(X[:50, 0], X[:50, 1],
            color='red', marker='o', label='setosa')
plt.scatter(X[50:100, 0], X[50:100, 1],
            color='blue', marker='x', label='versicolor')

plt.xlabel('sepal length [cm]') #x축의 이름 정하기
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

plt.show()#우리눈에 보여지는것
```


    
![png](output_36_0.png)
    


- 이렇게 가져온 데이터를 통해 퍼셉트론 알고리즘을 **훈련**해보자. 이때 에포크마다 잘못 분류된 오차를 그래프로 그려 확인해보자


```python
ppn = Perceptron(eta=0.1, n_iter=10)

ppn.fit(X, y)

plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Number of errors')

plt.show()
```


    
![png](output_38_0.png)
    


- 여기서 구해진 에러를 통해서 결정경계를 그려 시각화 시켜주자

결정 경계를 그릴때 두 특성의 최소,최댓값을 찾고 이 벡터로 넘파이 meshgrid 함수로 그리드 배열(바둑판 같은거) xx1, xx2쌍을 만든다. 이렇게 만든 그리드 배열을 분류기에 넘겨줘서 예측을 만들고 예측을 바탕으로 경계상자를 만든다. 


```python
from matplotlib.colors import ListedColormap


def plot_decision_regions(X, y, classifier, resolution=0.02):

    # 마커와 컬러맵을 설정합니다
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # 결정 경계를 그립니다
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
                #첫번째 파라미터와 두번째 파라미터를 범위로 잡고 세번째 값을 간격으로 잡고 어레이를 출력
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))#resolution은 떨어져 있는 경계를 나타냄
                                    #다차원 배열을 1차원으로 바꾸어 준다.
        
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    print(Z.shape)
    #등치선을 그려주는 함수이다.
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)
    # 축제한을 둔다.
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # 샘플의 산점도를 그립니다
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='black')
```


```python
plot_decision_regions(X, y, classifier=ppn)
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

plt.show()
```

    (305, 235)



    
![png](output_42_1.png)
    


## 2.3 적응형 선형 뉴런과 학습의 수렴

- 퍼셉트론 모델이 나온 후에 적응형 선형 뉴런이라는 새로운게 등장한다. 이것은 비용함수를 정의하고 최소화하는게 핵심이다. *비용함수는 앞으로 나올 여러 알고리즘에서 쓰일 매우 중요한 개념이다*


- 퍼셉트론과의 차이점은 가중치를 업데이트 하는데 단위계단함수 대신 선형 활성화 함수를 사용하는 것이다.

![image.png](attachment:image.png)

- 퍼셉트론은 실제 출력값을 진짜 클래스 레이블을 비교하는 것이고, 아달린은 활성화함수를 거친 출력값과 진짜 클래스 레이블을 비교하는 것이다.

### 2.3.1 경사하강법으로 비용 함수 최적화

- 학습 과정 동안 최적화하기 위해 정의한 목적함수는 종종 비용함수와 같게 취급된다. 아달린에서 사용한 비용함수이자 목적함수는 SSE(제곱 오차합)이다. SSE는 정답값과 예측한 값의 차이를 제곱한 값이다. 여기서는 이값을 최소화하는데 집중한다.

$$J(w)={1\over 2}\sum_i(y^{(i)}-\Phi(z^{(i)}))^2$$

- ${1\over 2}$는 그래디언트를 간소하게 만들려고 편의상 추가한 것이다. 퍼셉트론의 단위계단함수가 아니라 선형 활성화 함수를 사용한 경우에는 미분이 가능해진다는 큰 장점이 있다. 미분 가능하다는 장점을 이용한 최적화 알고리즘중 경사하강법이 있다. 경사하강법을 사용해 비용함수를 최소화하는 가중치를 찾을 수 있다. 

![image.png](attachment:image.png)

- 위의 그림과 같이 경사하강법은 말그대로 경사를 내려오면서 최소값까지 도달하는 것이다.

- 그러면 경사를 어떻게 내려오는지 과정 및 수식을 보자


경사를 따라 내려오는 것은 현재의 위치에 대해 미분한다. 미분한 값이 기울기가 급한 곳(절대값을 취한 미분값이 큰곳)은 값이 크기 때문에 많이 내려오고, 기울기가 완만한 곳은 값이 작기 때문에 적게 내려온다. 이것을 수식으로 표현하면 다음과 같다.


$$\Delta w =-\eta \nabla J(w)$$




$${\partial J \over \partial w_j}=-\sum_{i}(y^{(i)}-\Phi(z^{(i)}))x_j^{(i)}$$

$$\Delta w_j = -\eta{\partial J \over \partial w_j}=\eta \sum_{i}(y^{(i)}-\Phi(z^{(i)}))x_j^{(i)}$$

- 가중치는 $w = w+ \Delta w$를 통해 계산된다.

- 이렇게 하나 하나 샘플에 대해서 경사하강법을 진행하는 것을 배치 경사 하강법이라고 한다.


### 2.3.2 파이썬으로 아달린 구현


```python
class AdalineGD(object):
    """적응형 선형 뉴런 분류기

    매개변수
    ------------
    eta : float
      학습률 (0.0과 1.0 사이)
    n_iter : int
      훈련 데이터셋 반복 횟수
    random_state : int
      가중치 무작위 초기화를 위한 난수 생성기 시드

    속성
    -----------
    w_ : 1d-array
      학습된 가중치
    cost_ : list
      에포크마다 누적된 비용 함수의 제곱합

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        # 학습률, epoch, 시드 설정
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """훈련 데이터 학습

        매개변수
        ----------
        X : {array-like}, shape = [n_samples, n_features]
          n_samples 개의 샘플과 n_features 개의 특성으로 이루어진 훈련 데이터
        y : array-like, shape = [n_samples]
          타깃값

        반환값
        -------
        self : object

        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        # 비용함수 계산을 위한 리스트 초기화
        self.cost_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            # 퍼셉트론과는 달리 활성화 함수 추가
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.cost_.append(cost)
        return self

    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        """선형 활성화 계산"""
        return X

    def predict(self, X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
```

- 퍼셉트론과는 다르게 전체 훈련 데이터셋을 기반으로 그래디언트를 계산한다. 그리고 활성화함수 activation 메서드는 여기서는 간단하게 항등행렬로 정의했다.

- 1장에서 말한 하이퍼 파라미터를 수정해가면서 어떤 학습률이 학습에 적절한지 알아보도록 하자


```python
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)
ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Sum-squared-error)')
ax[0].set_title('Adaline - Learning rate 0.01')

ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)
ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Sum-squared-error')
ax[1].set_title('Adaline - Learning rate 0.0001')

plt.show()
```


    
![png](output_60_0.png)
    


- 학습률을 0.01로 했을때는 오히려 비용함수가 커지는 것을 볼수 있고 0.001로 했을때 비용함수가 점점 주는 것을 볼 수가 있다. 그렇다면 왜 학습을 할수록 비용함수가 커지는 것일까

- 학습률을 정할때 너무 크면 오히려 전역 최소값을 건너뛰게 되고 너무 작으면 전역최소값에 도달하는데 너무 오랜시간이 걸리게 된다.

![image.png](attachment:image.png)

### 2.3.3 특성 스케일을 조정하여 경사 하강법 결과 향상

- 특성의 스케일 다시말해 특성의 크기를 변환시켜 머신러닝 알고리즘의 성능을 향상 시킬 수 있다. 자세한거는 이후에 다룰 것이고, 이 장에서는 경사하강법에 표준화를 적용하는 것을 살펴보겠다. 표준화(standardization)은 특성이 평균0, 표준편차1을 가지는 정규분포로 바꾸는 것이다. 이 과정은 다음 식을 통해 수행된다
$$x_j'= {x_j-\mu _j \over \sigma _j}$$

- 각 변수의 스케일이 다르면 변화시켜야 되는 크기 또한 다르기 때문에 비용함수를 다르게 정의해주어야되고, 스케일이 다르면 학습속도에 영향을 주게 된다.

![image.png](attachment:image.png)


```python
# 특성을 표준화합니다.
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()
```

- 실행결과를 보면 스케일을 조정해주었을때 더 좋은 학습 성능을 보인다.


```python
ada = AdalineGD(n_iter=15, eta=0.01)
ada.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada)
plt.title('Adaline - Gradient Descent')
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Sum-squared-error')

plt.tight_layout()
plt.show()
```

    (243, 312)



    
![png](output_70_1.png)
    



    
![png](output_70_2.png)
    


### 2.3.4 대규모 머신러닝과 확률적 경사 하강법 

- 기존의 경사하강법은 전체데이터를 한번에 학습하기 때문에 학습할 데이터 많아지면 계산량이 매우 증가하게 된다. 이에 대한 대안으로 확률적 경사 하강법(Stochastic gradient descent)이 나왔다. 

$$\Delta w= \eta \sum_{i}(y^{(i)}-\Phi(z^{(i)}))x_j^{(i)}$$




$$\Delta w= \eta (y^{(i)}-\Phi(z^{(i)}))x_j^{(i)}$$


- 첫번째 수식은 기존의 배치 경사 하강법이고, 두번째 수식은 확률적 경사 하강법이다. 수식에서 보는것처럼 확률적 경사하강법은 각 훈련 샘플 하나마다 학습을 해준다. 가중치가 자주 업데이트 되기 때문에 수렴 속도가 빠르고 지역 최솟값에 잘 빠지지 않는다.


- 학습을 할 때 에포크마다 데이터를 섞어주어야 좋은 훈련 결과를 얻을 수 있다.



- 이런 확률적 경사 하강법은 온라인 학습에 유리하다. 온라인 학습은 데이터가 새로 들어올때 마다 학습을 하는 방법인데 이러한 온라인 학습에 유리하다.

- 기존의 경사하강법 코드를 약간 수정해서 사용하자. fit 메서드 안에서 각 훈련 샘플에 대해 가중치를 업데이트 하도록 하고, 가중치를 초기화 하지 않고 온라인 학습을 할 수 있는 partial fit 메서드를 추가한다. 


- 훈련 후에는 알고리즘이 수렴하는지 확인하려고 에포크마다 훈련 샘플의 평균 비용을 계산한다. 


- 그리고 같은 데이터에 대해서 학습하지 않도록 훈련샘플을 섞는 메서드를 만든다.


```python
class AdalineSGD(object):
    """ADAptive LInear NEuron 분류기

    Parameters
    ------------
    eta : float
      학습률 (0.0과 1.0 사이)
    n_iter : int
      훈련 데이터셋 반복 횟수
    shuffle : bool (default: True)
      True로 설정하면 같은 반복이 되지 않도록 에포크마다 훈련 데이터를 섞습니다
    random_state : int
      가중치 무작위 초기화를 위한 난수 생성기 시드

    Attributes
    -----------
    w_ : 1d-array
      학습된 가중치
    cost_ : list
      모든 훈련 샘플에 대해 에포크마다 누적된 평균 비용 함수의 제곱합

    """
    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
        # 학습률, epoch, 시드, 셔플여부를 설정
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False
        self.shuffle = shuffle #shuffle: 섞어주기 위해
        self.random_state = random_state
        
    def fit(self, X, y):
        """훈련 데이터 학습

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
          n_samples 개의 샘플과 n_features 개의 특성으로 이루어진 훈련 데이터
        y : array-like, shape = [n_samples]
          타깃 벡터

        반환값
        -------
        self : object

        """
        self._initialize_weights(X.shape[1])
        self.cost_ = []
        for i in range(self.n_iter):
            if self.shuffle:
                X, y = self._shuffle(X, y)
            cost = []
            for xi, target in zip(X, y):
                cost.append(self._update_weights(xi, target))
            avg_cost = sum(cost) / len(y)
            self.cost_.append(avg_cost)
        return self

    def partial_fit(self, X, y):
        """가중치를 다시 초기화하지 않고 훈련 데이터를 학습합니다"""
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0] > 1:
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
        else:
            self._update_weights(X, y)
        return self

    def _shuffle(self, X, y):
        """훈련 데이터를 섞습니다"""
        r = self.rgen.permutation(len(y))
        return X[r], y[r]
    
    def _initialize_weights(self, m):
        """랜덤한 작은 수로 가중치를 초기화합니다"""
        self.rgen = np.random.RandomState(self.random_state)
        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)
        self.w_initialized = True
        
    def _update_weights(self, xi, target):
        """아달린 학습 규칙을 적용하여 가중치를 업데이트합니다"""
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error
        cost = 0.5 * error**2
        return cost
    
    def net_input(self, X):
        """최종 입력 계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        """선형 활성화 계산"""
        return X

    def predict(self, X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
```


```python
ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1)
ada.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada)
plt.title('Adaline - Stochastic Gradient Descent')
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
plt.show()

plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Average Cost')

plt.tight_layout()
plt.show()
```

    (243, 312)



    
![png](output_77_1.png)
    



    
![png](output_77_2.png)
    


- 훈련결과를 보면 경사 하강법보다 더 빠르게 비용이 줄어드는 것을 확인 할 수 있다.
